{"nbformat":4,"nbformat_minor":5,"metadata":{"notebookPath":"aligner1-1-Copy1.ipynb","accelerator":"GPU","language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","name":"python","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python","version":"3.7.7","file_extension":".py"},"notebookId":"1957bfda-8677-43cf-9536-d0ccd3db7141","colab":{"name":"aligner.ipynb","provenance":[],"collapsed_sections":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"1a9837f9abf944e9b664aef960167acf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_cc172b600a314df9a92d3855e187ab59","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_06b82227f72f495687b30193a9f9b21f","IPY_MODEL_3927b5383c294514b42b8ed6b6320566","IPY_MODEL_44182bcdf7944207b799804f82c7050c"]}},"cc172b600a314df9a92d3855e187ab59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"06b82227f72f495687b30193a9f9b21f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2a0feba5ba5d4469bccfffcf79537e0a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2096da11a7854130b82926aadd17f812"}},"3927b5383c294514b42b8ed6b6320566":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ce7f66dbf5fb4721a506435528e9908a","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":377664473,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":377664473,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_78e7f95e43f04dabab5b9c38a3ef9aca"}},"44182bcdf7944207b799804f82c7050c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_52eb03d459ab41728798f122a2f0b9e3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 360M/360M [00:05&lt;00:00, 65.8MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8990bcf1a7ad4cde96480eda7de9a537"}},"2a0feba5ba5d4469bccfffcf79537e0a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2096da11a7854130b82926aadd17f812":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ce7f66dbf5fb4721a506435528e9908a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"78e7f95e43f04dabab5b9c38a3ef9aca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"52eb03d459ab41728798f122a2f0b9e3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8990bcf1a7ad4cde96480eda7de9a537":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"kernelspec":{"name":"python3","display_name":"Yandex DataSphere Kernel","language":"python"}},"cells":[{"cell_type":"markdown","source":"## Download LJSpeech","metadata":{"cellId":"8v0red2e3f9c6hgv79ly3b","id":"0bcca84b"}},{"cell_type":"code","source":"!wget https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\n!tar -xjf LJSpeech-1.1.tar.bz2","metadata":{"outputId":"51e3b04c-d19a-4a50-931d-0831ffc533e0","id":"262cdc28","cellId":"ze6f2whpf9mylj252ovq9s","trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[],"execution_count":260},{"cell_type":"code","source":"%pip install librosa","metadata":{"outputId":"e4a900ec-fea2-4464-a084-cc22f5cd2168","id":"6703f3e3","cellId":"2aon69j3tgwswvpui2dp8k","trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[],"execution_count":261},{"cell_type":"code","source":"#!g1.1\n%pip install torch==1.10.0+cu111 torchaudio==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html","metadata":{"outputId":"4589bdb1-a722-4a54-d50a-b6c6712b919b","id":"e642b7e2","cellId":"fv6w4zu8ermne0vz8cyns","trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[],"execution_count":262},{"cell_type":"code","source":"#!git clone https://github.com/darya-baranovskaya/TTS_with_FastSpeach.git","metadata":{"outputId":"ea6f6608-ce67-4449-de35-37998be30c01","id":"VpxKRaKj1z_h","cellId":"e3klx3a4jvoq2r6i3ypvxi","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\n!git clone https://github.com/NVIDIA/waveglow.git\n%pip install googledrivedownloader","metadata":{"outputId":"ce58c3de-7cd0-49ac-b7ec-88ec5cbd77d7","id":"xhnaOx08LjnB","cellId":"9u2s8vhw3hc0c9mi38tty7u","trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[],"execution_count":263},{"cell_type":"code","source":"#!g1.1\n%pip install wandb","metadata":{"cellId":"y1jfthba3hdq9j210cska","trusted":true},"outputs":[],"execution_count":265},{"cell_type":"code","source":"#!g1.1\nimport wandb","metadata":{"cellId":"0jsh3ahyfw6ufxl9swwhayo","trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"#!g1.1\nfrom typing import Tuple, Dict, Optional, List, Union\nfrom itertools import islice\n\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence","metadata":{"id":"MPGMItqe5IAy","cellId":"tosn6wykufbm8i2iwdn6v","trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"#!g1.1\n%load_ext autoreload\n%autoreload 2","metadata":{"id":"HeAt6we5EDLI","cellId":"bar73iyqq4edz2vkk0cskd","trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"#!g1.1\nfrom models.fastspeech_model import FastSpeech\nfrom utils.batch_sampler import Batch\nfrom utils.ljspeech_dataset import LJSpeechDataset#, LJSpeechCollator\nfrom utils.melspectrogram import MelSpectrogram\n\nfrom configs.melspectrogram_config import MelSpectrogramConfig\nfrom models.vocoder import Vocoder\nfrom models.grapheme_aligner import GraphemeAligner, Point, Segment\n\nfrom typing import Tuple, Dict, Optional, List, Union\nfrom itertools import islice\n\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom IPython import display\nfrom dataclasses import dataclass\n\nimport torch\nfrom torch import nn\n\nimport torchaudio\n\nimport librosa\nfrom matplotlib import pyplot as plt\n\nimport warnings\nimport sys\nsys.path.append('waveglow/')\n\nwarnings.filterwarnings('ignore')","metadata":{"outputId":"fd93ad8d-71a2-404a-c7dc-396b837a99bc","id":"r3a04aadCxSZ","cellId":"rqnw2kz9svv6wm3g14zf","trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":380}},"outputs":[],"execution_count":683},{"cell_type":"markdown","source":"---","metadata":{"cellId":"5k7ilu0ok8wg3b9ypodj4s","id":"9fd361a1"}},{"cell_type":"markdown","source":"## Dataset","metadata":{"cellId":"g3mc8nkmkxstjatc04x0t","id":"a5827f24"}},{"cell_type":"code","source":"#!g1.1\ndataset = LJSpeechDataset('.')","metadata":{"id":"3b179b68","cellId":"fn2mmg4tt0rhwyni486fu6","trusted":true},"outputs":[],"execution_count":661},{"cell_type":"code","source":"#!g1.1\ndataset[0]","metadata":{"outputId":"c04e31ed-bbdb-4160-c849-66ddac04958b","id":"4f585ec7","cellId":"dupbvq8hqeo1ps2ts7pjn","trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[],"execution_count":273},{"cell_type":"code","source":"#!g1.1\ndevice = torch.device('cuda:0')\naligner = GraphemeAligner().to(device)\nfeaturizer = MelSpectrogram(MelSpectrogramConfig())\nclass LJSpeechCollator:\n\n    def __call__(self, instances: List[Tuple]) -> Dict:\n        waveform, waveforn_length, transcript, tokens, token_lengths = list(\n            zip(*instances)\n        )\n\n        waveform = pad_sequence([\n            waveform_[0] for waveform_ in waveform\n        ]).transpose(0, 1)\n        waveforn_length = torch.cat(waveforn_length)\n\n        tokens = pad_sequence([\n            tokens_[0] for tokens_ in tokens\n        ]).transpose(0, 1)\n        token_lengths = torch.cat(token_lengths)\n        batch = Batch(waveform, waveforn_length, transcript, tokens, token_lengths)\n        batch.melspec = featurizer(batch.waveform)\n        lengths = []\n        for i in range(batch.melspec.shape[0]):\n            lengths.append(featurizer(batch.waveform[i:i + 1, :batch.waveforn_length[i]]).shape[-1])\n        lengths = torch.Tensor(lengths).unsqueeze(1)\n        alignes = aligner(\n            batch.waveform.to(device), batch.waveforn_length, batch.transcript\n        )\n        batch.durations = lengths * alignes\n        return batch","metadata":{"cellId":"ty3ssz9eq0ou6rmrf2eybj","trusted":true},"outputs":[],"execution_count":839},{"cell_type":"code","source":"#!g1.1\ndataloader = DataLoader(LJSpeechDataset('.'), batch_size=3, collate_fn=LJSpeechCollator())","metadata":{"id":"4e451bf0","cellId":"umnaib2ii08rj194r5c6tm","trusted":true},"outputs":[],"execution_count":840},{"cell_type":"code","source":"#!g1.1\nbatch = next(iter(dataloader))\nbatch.tokens.shape","metadata":{"outputId":"6e8f616d-64a0-4519-a672-42466f6bc6e9","id":"sunEM0HHAc6j","cellId":"7aqotszuwhr960ws9crrkw","trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[],"execution_count":720},{"cell_type":"code","source":"#!g1.1\nbatch.__dict__.keys()","metadata":{"outputId":"78478e10-8fca-4659-fe55-d3c84884bd1f","id":"ZuphJmxjE_a9","cellId":"p700hjg8dytza4eyd5zbn","trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[],"execution_count":140},{"cell_type":"code","source":"#!g1.1\nprint(batch.durations.shape)\nprint(batch.waveform.shape) \nprint(batch.waveforn_length)\nprint(len(batch.transcript), batch.transcript[0])\nprint(batch.tokens.shape)\nprint(batch.token_lengths)\nprint(batch.melspec.shape)","metadata":{"outputId":"94ce10aa-d81c-403d-fc36-0257f7aecbbd","id":"gavwaIJoGVD7","cellId":"shb5xqbt0fknyb4usxna7","trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[],"execution_count":141},{"cell_type":"code","source":"#!g1.1\ndummy_batch = list(islice(dataloader, 1))[0]","metadata":{"cellId":"o10nh6c6dcdiqol7u0knte","trusted":true},"outputs":[],"execution_count":117},{"cell_type":"markdown","source":"---","metadata":{"cellId":"ytvgketpd8a6qxsu20dmm","id":"38df0f5a"}},{"cell_type":"markdown","source":"## Vocoder","metadata":{"cellId":"lgxo3nxjg53jnfikzhf4i","id":"71aaa2bc"}},{"cell_type":"code","source":"#!g1.1\nfrom google_drive_downloader import GoogleDriveDownloader as gdd","metadata":{"id":"8461bac1","cellId":"ygoltlh4lbkhdroyjomjbf","trusted":true},"outputs":[],"execution_count":681},{"cell_type":"code","source":"#!g1.1\ngdd.download_file_from_google_drive(\n    file_id='1rpK8CzAAirq9sWZhe9nlfvxMF1dRgFbF',\n    dest_path='./waveglow_256channels_universal_v5.pt'\n)","metadata":{"id":"47a84237","cellId":"hue8w0pw5xhseytzuvx5y","trusted":true},"outputs":[],"execution_count":682},{"cell_type":"code","source":"#!g1.1\nfrom models.vocoder import Vocoder","metadata":{"id":"Pb-XMLnFRsaD","cellId":"5t7ptksanfxgwvae8jikz","trusted":true},"outputs":[],"execution_count":841},{"cell_type":"code","source":"#!g1.1\nvocoder = Vocoder().to('cuda:0').eval()","metadata":{"outputId":"373eb782-b0c2-4f3b-deb9-d44bfd43d975","id":"338d1f3a","cellId":"e7ydqi3nq7vx2qtm7vxcuf","trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[],"execution_count":842},{"cell_type":"markdown","source":"---","metadata":{"cellId":"xlz2vmz9879h6mx54j4tar","id":"678ae438"}},{"cell_type":"markdown","source":"# Batch overfit","metadata":{"cellId":"mfzbpaodt4s04dbb4jxj8"}},{"cell_type":"code","source":"#!g1.1\nfrom models.fastspeech_model import *\nfrom models.model_layers import *\nclass ModelConfig:\n    vocab_size: int = 1000\n    hidden_size: int = 384\n    hidden_size_fft: int = 1536\n    num_heads: int = 2\n    kernel_size: int = 3\n    n_fft_blocks: int = 2\n    dropout = 0.1\n        \nmodel = FastSpeech(ModelConfig)\nmodel = model.to(device)\n","metadata":{"cellId":"ioahh2dj0y8klc7ubel30o","trusted":true},"outputs":[],"execution_count":730},{"cell_type":"code","source":"#!g1.1\nreconstructed_wav = vocoder.inference(batch.melspec.to(device)).cpu()","metadata":{"cellId":"fm0drk599km9hrhvss7bpu","trusted":true},"outputs":[],"execution_count":489},{"cell_type":"code","source":"#!g1.1\ndisplay.display(display.Audio(reconstructed_wav[0], rate=22050))\ndisplay.display(display.Audio(reconstructed_wav[1], rate=22050))\ndisplay.display(display.Audio(reconstructed_wav[2], rate=22050))","metadata":{"cellId":"9ql5utwhkrt9k0f1e3hsan","trusted":true},"outputs":[],"execution_count":490},{"cell_type":"code","source":"#!g1.1\nwandb.init(name='Batch_overfit1_Sequential_2fft')\nwandb.log({'batch_text0': wandb.Html(batch.transcript[0])})\nwandb.log({'batch_text1': wandb.Html(batch.transcript[1])})\nwandb.log({'batch_text2': wandb.Html(batch.transcript[2])})\n\nN_ITERATIONS = 10000\nmodel.train()\n\nloss_fn_mel = nn.L1Loss()\n# loss_fn_align = nn.L1Loss()\nloss_fn_align = nn.MSELoss(reduction='mean')\noptimizer = torch.optim.Adam(model.parameters(), betas=(0.9, 0.98),\n                                 eps=1e-9)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.98999)\n# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=N_ITERATIONS//10, epochs=10)\n\n\nmel_losses = 0\nalign_losses = 0\n\nfor i in range(N_ITERATIONS):\n    inp = batch.tokens.to(device)\n    gt = batch.melspec.to(device)\n    align_gt = batch.durations.to(device)\n#     align_gt = (batch.durations + 0.5).type(torch.LongTensor).to(device)\n\n    mel, align = model(inp, align_gt)\n\n    optimizer.zero_grad()\n    \n    if mel.shape[-1] < gt.shape[-1]:\n        mel = torch.cat([mel, -11.5129 * torch.ones((mel.shape[0], 80, gt.shape[-1] - mel.shape[-1])).to(device)], dim=-1)\n    elif mel.shape[-1] > gt.shape[-1]:\n        with torch.no_grad():\n            gt = torch.cat([gt, -11.5129 * torch.ones((gt.shape[0], 80, mel.shape[-1] - gt.shape[-1])).to(device)], dim=-1)\n    mel_loss = loss_fn_mel(mel, gt)\n#     print(align)\n    align_loss = loss_fn_align(align, align_gt)\n    loss = mel_loss + align_loss\n    loss.backward()\n\n    optimizer.step()\n    scheduler.step()\n    mel_losses += mel_loss.item()\n    align_losses += align_loss.item()\n    wandb.log({'train_mel_loss': mel_loss, 'train_align_loss': align_loss, 'train_loss': loss})\n    if i % 500 == 0:\n        with torch.no_grad():\n            reconstructed_wav = vocoder.inference(mel.to(device)).cpu()\n            print(f\"step {i} sampled_audios\")\n            display.display(display.Audio(reconstructed_wav[0], rate=22050))\n            wandb.log({'reconstructed_wav0': wandb.Audio(reconstructed_wav[0].detach().cpu().numpy(), sample_rate=22050),\n                      'reconstructed_wav1': wandb.Audio(reconstructed_wav[1].detach().cpu().numpy(), sample_rate=22050),\n                      'reconstructed_wav2': wandb.Audio(reconstructed_wav[2].detach().cpu().numpy(), sample_rate=22050),\n                      'melspec0': wandb.Image(mel[0].detach().cpu().numpy()),\n                      'melspec1': wandb.Image(mel[1].detach().cpu().numpy()),\n                      'melspec2': wandb.Image(mel[2].detach().cpu().numpy())})","metadata":{"cellId":"5vvtfmxysc9adi65zknj3s","trusted":true},"outputs":[],"execution_count":722},{"cell_type":"code","source":"#!g1.1\nprint(\"Результат переобучения на батче за 10к шагов с собственным alignment\")\nmodel.eval()\ninp = batch.tokens.to(device)\ngt = batch.melspec.to(device)\nalign_gt = batch.durations.to(device) # revert that with torch.exp(torch.log(align_gt + 1)) - 1 \n\nmel, align = model(inp)\nreconstructed_wav = vocoder.inference(mel.to(device)).cpu()\nprint(f\"step {i} sampled_audios\")\nwandb.log({'reconstructed_wav0_eval': wandb.Audio(reconstructed_wav[0].detach().cpu().numpy(), sample_rate=22050),\n          'reconstructed_wav1_eval': wandb.Audio(reconstructed_wav[1].detach().cpu().numpy(), sample_rate=22050),\n          'reconstructed_wav2_eval': wandb.Audio(reconstructed_wav[2].detach().cpu().numpy(), sample_rate=22050),\n          'melspec0_eval': wandb.Image(mel[0].detach().cpu().numpy()),\n          'melspec1_eval': wandb.Image(mel[1].detach().cpu().numpy()),\n          'melspec2_eval': wandb.Image(mel[2].detach().cpu().numpy())})\ndisplay.display(display.Audio(reconstructed_wav[0], rate=22050))","metadata":{"cellId":"2qzine44iz8wai84pldolq","trusted":true},"outputs":[],"execution_count":564},{"cell_type":"markdown","source":"# Train\n","metadata":{"cellId":"xz9ouipeppbs7gwjy2mu5"}},{"cell_type":"code","source":"#!g1.1\ndevice = torch.device('cuda:0')\ndef train_epoch(model, optimizer, dataloader, scheduler, loss_fn_mel, loss_fn_align):\n    model.train()\n    mel_losses = 0\n    align_losses = 0\n    for batch in dataloader:\n        inp = batch.tokens.to(device)\n        gt = batch.melspec.to(device)\n        align_gt = batch.durations.to(device)\n    #     align_gt = (batch.durations + 0.5).type(torch.LongTensor).to(device)\n        if inp.shape[-1] != align_gt.shape[-1]:\n#             print(inp.shape, align_gt.shape)\n            min_shape = min(inp.shape[-1], align_gt.shape[-1])\n            inp = inp[:, :min_shape]\n            align_gt = align_gt[:, :min_shape]\n        mel, align = model(inp, align_gt)\n\n        optimizer.zero_grad()\n\n        if mel.shape[-1] < gt.shape[-1]:\n            mel = torch.cat([mel, -11.5129 * torch.ones((mel.shape[0], 80, gt.shape[-1] - mel.shape[-1])).to(device)], dim=-1)\n        elif mel.shape[-1] > gt.shape[-1]:\n            with torch.no_grad():\n                gt = torch.cat([gt, -11.5129 * torch.ones((gt.shape[0], 80, mel.shape[-1] - gt.shape[-1])).to(device)], dim=-1)\n        mel_loss = loss_fn_mel(mel, gt)\n    #     print(align)\n        align_loss = loss_fn_align(align, align_gt)\n        loss = mel_loss + align_loss\n        loss.backward()\n\n        optimizer.step()\n        scheduler.step()\n        \n        mel_losses += mel_loss.item()\n        align_losses += align_loss.item()\n    wandb.log({'train_mel_loss':  mel_losses / len(dataloader), 'train_align_loss': align_losses/len(dataloader), 'train_loss': loss,\n              'optimizer_lr': optimizer.param_groups[0]['lr']})\n    with torch.no_grad():\n        reconstructed_wav = vocoder.inference(mel.to(device)).cpu()\n        print(f\"train sampled_audios\")\n        display.display(display.Audio(reconstructed_wav[0], rate=22050))\n        for i in range(reconstructed_wav.shape[0]):\n            wandb.log({f'train_text{i}': wandb.Html(batch.transcript[i]),\n                       f'reconstructed_train_wav{i}': wandb.Audio(reconstructed_wav[i].detach().cpu().numpy(), sample_rate=22050),\n                       f'melspec_train_{i}': wandb.Image(mel[i].detach().cpu().numpy())})\n    return align_losses/len(dataloader), mel_losses / len(dataloader)","metadata":{"cellId":"wzn6ai30c4ptjpbob4a1","trusted":true},"outputs":[],"execution_count":844},{"cell_type":"code","source":"#!g1.1\ndevice = torch.device('cuda:0')\ndef validate(model, dataloader, loss_fn_mel, loss_fn_align):\n    model.eval()\n    mel_losses = 0\n    align_losses = 0\n    for batch in dataloader:\n        with torch.no_grad():\n            inp = batch.tokens.to(device)\n            gt = batch.melspec.to(device)\n            align_gt = batch.durations.to(device)\n            \n            mel, align = model(inp)\n            if mel.shape[-1] < gt.shape[-1]:\n                mel = torch.cat([mel, -11.5129 * torch.ones((mel.shape[0], 80, gt.shape[-1] - mel.shape[-1])).to(device)], dim=-1)\n            elif mel.shape[-1] > gt.shape[-1]:\n                with torch.no_grad():\n                    gt = torch.cat([gt, -11.5129 * torch.ones((gt.shape[0], 80, mel.shape[-1] - gt.shape[-1])).to(device)], dim=-1)\n            \n            mel_loss = loss_fn_mel(mel, gt)\n            align_loss = loss_fn_align(align, align_gt)\n            loss = mel_loss + align_loss\n            \n            mel_losses += mel_loss.item()\n            align_losses += align_loss.item()\n    with torch.no_grad():\n        reconstructed_wav = vocoder.inference(mel.to(device)).cpu()\n        print(f\"validation sampled_audios\")\n        display.display(display.Audio(reconstructed_wav[0], rate=22050))\n        for i in range(reconstructed_wav.shape[0]):\n            wandb.log({f'val_text{i}': wandb.Html(batch.transcript[i]),\n                       f'reconstructed_val_wav{i}': wandb.Audio(reconstructed_wav[i].detach().cpu().numpy(), sample_rate=22050),\n                       f'melspec_val_{i}': wandb.Image(mel[i].detach().cpu().numpy())})\n    wandb.log({'val_mel_loss': mel_losses / len(dataloader),\n               'val_align_loss': align_losses/len(dataloader), 'val_loss': (mel_losses + align_losses)/len(dataloader)})\n    return align_losses/len(dataloader), mel_losses / len(dataloader)\n","metadata":{"cellId":"ggwdv6y6ec8kp56yzpger","trusted":true},"outputs":[],"execution_count":757},{"cell_type":"code","source":"#!g1.1\ntokenizer = torchaudio.pipelines.TACOTRON2_GRIFFINLIM_CHAR_LJSPEECH.get_text_processor()\ndef test(model):\n    model.eval()\n    test_texts = ['A defibrillator is a device that gives a high energy electric shock to the heart of someone who is in cardiac arrest',\n                 'Massachusetts Institute of Technology may be best known for its math, science and engineering education',\n                 'Wasserstein distance or Kantorovich Rubinstein metric is a distance function defined between probability distributions on a given metric space']\n    tokens = [tokenizer(text)[0] for text in test_texts]\n    for i, token in enumerate(tokens):\n        token = token.to(device)\n        mel, align = model(token)\n        reconstructed_wav = vocoder.inference(mel.to(device)).cpu()\n        print(f\"test sampled_audios\")\n        display.display(display.Audio(reconstructed_wav[0], rate=22050))\n        wandb.log({f'reconstructed_test_wav{i}': wandb.Audio(reconstructed_wav[0].detach().cpu().numpy(), sample_rate=22050),\n                  f'melspec_val_{i}': wandb.Image(mel[0].detach().cpu().numpy()),\n                  f'test_text{i}':wandb.Html(test_texts[i])})","metadata":{"cellId":"e9l3by4b7w99ucc8py9b3","trusted":true},"outputs":[],"execution_count":775},{"cell_type":"code","source":"#!g1.1\ndataset = LJSpeechDataset('.')\ndevice = torch.device('cuda:0')\naligner = GraphemeAligner().to(device)\nfeaturizer = MelSpectrogram(MelSpectrogramConfig())\n\ntorch.manual_seed(3407)\ntest_size = 30\ntrain_size = len(dataset) -  test_size#int(0.95 * len(dataset))\n# test_size = len(dataset) - train_size\ntrain_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])","metadata":{"cellId":"4whp2dk6ypptd7amopv6h","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\ntrain_dataloader = DataLoader(train_dataset, batch_size=10, collate_fn=LJSpeechCollator())\nval_dataloader = DataLoader(test_dataset, batch_size=3, collate_fn=LJSpeechCollator())","metadata":{"cellId":"erke4vn4zyo6nyq6bd1v0d","trusted":true},"outputs":[],"execution_count":848},{"cell_type":"code","source":"#!g1.1\nlen(train_dataloader), len(val_dataloader)","metadata":{"cellId":"7etaqu4htowdy62z3nsej5","trusted":true},"outputs":[],"execution_count":849},{"cell_type":"code","source":"#!g1.1\nfrom models.model_layers import *\nfrom models.fastspeech_model import FastSpeechEncoder, FastSpeechDecoder, FastSpeech\n\nclass ModelConfig:\n    vocab_size: int = 57\n    hidden_size: int = 384\n    hidden_size_fft: int = 1536\n    num_heads: int = 2\n    kernel_size: int = 3\n    n_fft_blocks: int = 6\n    dropout = 0.1\n        \nmodel = FastSpeech(ModelConfig)\nmodel = model.to(torch.device('cuda:0'))","metadata":{"cellId":"qr97kl2y0pi8jtglwx6eir","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\nfrom tts.model.fastspeech import FastSpeech","metadata":{"cellId":"nn08nrfck9epvsae66p8i"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\nscheduler","metadata":{"cellId":"3tq3ebe2jiee30buae28p","trusted":true},"outputs":[],"execution_count":847},{"cell_type":"code","source":"#!g1.1\naligner = GraphemeAligner().to(torch.device('cuda:0'))\n\nN_EPOCHS = 60\nRUN_NAME = 'model_nfft-6_hidden1-384_hidden2-1536_dropout-0.1'\nwandb.init(name=RUN_NAME)\n\n\n\nloss_fn_mel = nn.L1Loss()\nloss_fn_align = nn.MSELoss(reduction='mean')\noptimizer = torch.optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-9, lr = 0.00001)\n# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=len(train_dataloader) // 5, gamma=0.98999)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.0005, steps_per_epoch=len(train_dataloader), epochs=N_EPOCHS)\n# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0008, max_lr=0.01,\n#                                               step_size_up=len(train_dataloader), step_size_down=5*len(train_dataloader),\n#                                               cycle_momentum=False)\n\n\nfor epoch in range(23, N_EPOCHS):\n    train_epoch(model, optimizer, train_dataloader, scheduler, loss_fn_mel, loss_fn_align)\n    print(f'epoch {epoch} ended')\n    validate(model, val_dataloader, loss_fn_mel, loss_fn_align)\n    test(model)\n    torch.save(model.state_dict(), RUN_NAME + '_last_epoch.pth')\n    if epoch % 5 == 0 and epoch > 20 :\n        torch.save(model.state_dict(), RUN_NAME + '_epoch_' + str(epoch) + '.pth')\n","metadata":{"cellId":"h4uvvoc82z50d9rxeavn4","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\n","metadata":{"cellId":"6o2k1d4yltld9l792bxwwh"},"outputs":[],"execution_count":null}]}